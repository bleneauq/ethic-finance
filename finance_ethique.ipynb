{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c742b6d0",
   "metadata": {},
   "source": [
    "# Optimisation de patrimoine en 4 dimensions : rendement risque, risque extrême, éthique.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26169935",
   "metadata": {},
   "source": [
    "## Introduction :\n",
    "\n",
    "Aujourd'hui, la gestion de patrimoine évolue selon les exigences croissantes des investisseurs. Ces dernier cherchent de plus en plus des approches plus éthiques et responsables dans leurs investissements. Dans un monde où les préoccupations financières traditionnelles se mêlent à la nécessité de faire face aux risques de tout genre, qu'ils soient extrêmes, climatiques, etc. notre projet vise à offrir une expertise transparente pour aider les épargnants à optimiser leur patrimoine. L'objectif principal est d'assurer leurs besoins futurs tout en intégrant des considérations éthiques, et ce, dans un contexte de transformation du monde par des financements \"coup de cœur\".\n",
    "\n",
    "Le projet repose sur une approche en quatre dimensions, alliant rendement, risque, éthique, et une prévoyance spécifique face aux risques extrêmes. La dimension éthique introduit une perspective unique en évaluant chaque actif selon des critères éthiques spécifiques, permettant ainsi aux investisseurs de personnaliser leur portefeuille en fonction de leur valeurs.\n",
    "\n",
    "### Étape 1 : Univers d'investissement éthique\n",
    "\n",
    "La première étape consiste à construire un univers d'investissement éthique en évaluant chaque actif selon des critères éthiques définis. Ce processus implique une collaboration entre les étudiants pour attribuer une note éthique à chaque actif, créant ainsi un ensemble diversifié d'investissements alignés sur des valeurs partagées.\n",
    "\n",
    "### Étape 2 : Optimisation en 3D\n",
    "\n",
    "Dans cette phase, nous explorons différentes méthodes d'optimisation sous contraintes, intégrant des objectifs financiers et éthiques. En utilisant des cours historiques provenant de sources comme Yahoo Finance, nous testons des scénarios aléatoires de rendements. En investissant 10 000 €, une partie substantielle est protégée par des achats d'obligations d'État, tandis que le reste est réparti dans l'univers d'investissement défini. Nous évaluons également l'impact sur le portefeuille en introduisant des contraintes liées à l'inflation et à des notes éthiques minimales.\n",
    "\n",
    "### Étape 3 : Optimisation en 4D\n",
    "\n",
    "La dernière étape étend l'optimisation en intégrant la possibilité d'investir dans l'or et le Franc suisse pour atténuer les pertes en cas de krach. Des scénarios spécifiques définissent les variations des actifs pendant un krach, permettant une gestion proactive des risques extrêmes.\n",
    "\n",
    "Ce projet s'inscrit dans une nouvelle ère de gestion de patrimoine, où les considérations éthiques, la gestion du risque et l'innovation financière convergent pour offrir une approche plus complète et personnalisée. En examinant ces quatre dimensions, nous aspirons à fournir une méthodologie robuste pour aider les investisseurs à naviguer dans un paysage financier complexe, tout en restant fidèles à leurs valeurs et en se préparant à des éventualités extraordinaires."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ba17c0",
   "metadata": {},
   "source": [
    "Ce notebook mêlera ébauches de code, ainsi que des conclusions quant à nos recherches et nos observations. Quant au travail réalisé, nous avons en grande partie réalisé le travail ensemble, mais concernant les grandes parties, Nassim s'est occupée de la partie sur les notations  ESG, et de la dernière partie sur la modélisation d'un krach. Quentin s'est quant à lui occupé de la partie optimisation ESG et donc des différentes méthodes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510182c3",
   "metadata": {},
   "source": [
    "## Étape 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d0f4e1",
   "metadata": {},
   "source": [
    "Pour construire notre univers d'investissement éthique nous nous sommes appuyer sur le site INVESTIR ETHIQUE. L'objectif de ce dernier est donner aux épargnants les clés pour faire fluctifier leur épargne, tout en respectant leurs convictions environnementales et sociales. Le site nous indique même que 60% des français sont intéressés par l'investissement responsable. Ce chiffre nous montre bien l'intérêt de traiter un tel sujet et la volonté des investisseurs à promouvoir les entreprises responsables, les fonds d'investissements verts, ou des entreprises proposant des projets d'énergies renouvelables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "858851ca",
   "metadata": {},
   "source": [
    "Voici un aperçu du csv que nous avons créé en répertoriant les notations ESG des entreprises du CAC40. Le fichier csv classe dans l'ordre décroissant celles-ci selon leur note éthique finale, moyennant leurs notes dans les catégories Environnement, Social et Gouvernance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d83504",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "notes_ethiques = pd.read_csv('note_éthique.csv')\n",
    "scores_ethiques = notes_ethiques.set_index('Entreprise')['Score global']\n",
    "notes_ethiques.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c91c3d6",
   "metadata": {},
   "source": [
    "À présent grâce à la librairie python yfinance (Yahoo finance), nous allons récupérer les données historiques des différentes entreprises du CAC40"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df750995",
   "metadata": {},
   "source": [
    "Pour rendre nos investissements et nos expérimentations plus simples, nous avons donc choisi de collecter les données depuis 2020 pour les 10 entreprises du CAC 40. On peut par ailleurs observer que le cours de l'action s'est considérérablement en plus de 10 ans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d81b9b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "companies_names = [\"Sanofi\", \"Air Liquide\", \"Thalès\", \"L'Oréal\", \"BNP\", \"Airbus\", \"Danone\", \"Axa\", \"Safran\", \"Total\"]\n",
    "companies_ticker = [\"SAN.PA\", \"AI.PA\", \"HO.PA\", \"OR.PA\", \"BNP.PA\", \"AIR.PA\", \"BN.PA\", \"CS.PA\", \"SAF.PA\", \"TTE.PA\"]\n",
    "\n",
    "prices = yf.download(companies_ticker, start=\"2020-01-31\", end=\"2024-01-15\")[\"Close\"]\n",
    "\n",
    "# Converting datetime format\n",
    "prices.reset_index(inplace=True)\n",
    "prices['Date'] = prices['Date'].dt.strftime('%Y/%m/%d')\n",
    "prices['Date'] = pd.to_datetime(prices['Date'])\n",
    "prices.set_index('Date', inplace=True)\n",
    "\n",
    "prices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c545fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On interpole les données manquantes\n",
    "prices = prices.interpolate(method=\"nearest\")\n",
    "prices.tail().round(1)\n",
    "\n",
    "# On calcule les rendements\n",
    "sample_returns = prices.pct_change().dropna()\n",
    "sample_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6708a5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ethic_scores = notes_ethiques[notes_ethiques['Entreprise'].isin(companies_names)].set_index('Entreprise').reindex(companies_names)\n",
    "\n",
    "sample_scores = sample_ethic_scores[\"Score global\"]\n",
    "print(sample_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28899d3b",
   "metadata": {},
   "source": [
    "Nous allons travailler avec un dictionaire réportoriants les différentes dimensions qui nous servirons pour construire notre portefeuille optimal, à savoir le risque ( ou l'écart-type des returns ), la rentabilité ( l'espérances de ces derniers) ainsi que le score ESG. On va travailler avec les rentabilité, volatilitées annuelles. Voici ce à quoi vont ressembler nos rentabilitées annuelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf34dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk-Return profile\n",
    "ret_stats = sample_returns.agg(['mean','std']).T\n",
    "ret_stats.columns=['Return','Risk']\n",
    "ret_stats['Return'] = ret_stats['Return']*252\n",
    "ret_stats['Risk'] = ret_stats['Risk']*np.sqrt(252)\n",
    "ret_stats['Sharpe_Ratio']=(ret_stats['Return']-0.01)/ret_stats['Risk'] # rf: US 10y Bond Yield at January 31,2023\n",
    "ret_stats = ret_stats.sort_values('Sharpe_Ratio', ascending=False)\n",
    "ret_stats.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23221a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting annualized Risk, Return and Sharpe Ratio\n",
    "from adjustText import adjust_text\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.scatter(data=ret_stats,x='Risk',y='Return', c='Sharpe_Ratio', cmap='viridis')\n",
    "plt.colorbar(label='Sharpe Ratio')\n",
    "texts = []\n",
    "for x, y, s in zip(ret_stats['Risk'],ret_stats['Return'], ret_stats.index):\n",
    "    texts.append(plt.text(x,y,s))\n",
    "adjust_text(texts)\n",
    "plt.xlabel('Annualized Risk'), plt.ylabel('Annualized Return')\n",
    "plt.title('Risk-Return profile', weight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4f5283",
   "metadata": {},
   "source": [
    "Le ratio de Sharpe se définit comme le rendement excédentaire des actions (ou du portefeuille) divisé par l'écart type des rendements. On remarque facilement que L'Oréal a le meilleur ratio. On le remarque également facilement dans la graphique ci-dessus, car il s'agit du stock avec le rendement le plus pour un niveau de risque faible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2af007b",
   "metadata": {},
   "source": [
    "Étant donné que tous les stocks ont un bêta inférieur à un, nous pouvons dire qu'ils ont tous une volatilité inférieure à celle du marché, c'est-à-dire que l'inclusion de chacun d'entre eux dans un portefeuille le rend moins risqué que le même portefeuille sans cette action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecbe32af",
   "metadata": {},
   "source": [
    "De même, à titre d'indication pour la suite, nous pouvons nous intéresser aux rendements journaliers cumulatifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2bb7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation of cumulative returns, as the growth of £1 \n",
    "cumulative_returns = ((1 + sample_returns).cumprod()-1)*100\n",
    "\n",
    "# Rates of change for each company share\n",
    "ticker_change = cumulative_returns.tail(1).round(3)\n",
    "ticker_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128defd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def line_plot(data,legend_loc, legend_ncol, title, ylabel):\n",
    "    plt.figure(figsize=(15,5))\n",
    "    for col in data:\n",
    "        plt.plot(data.index, data[col], marker='', linewidth=1, label=col)\n",
    "        plt.legend(loc=legend_loc, ncol=legend_ncol)\n",
    "        plt.margins(x=0)\n",
    "        plt.title(title, weight='bold')\n",
    "        plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "    \n",
    "# Plotting daily cummulative returns\n",
    "line_plot(data=cumulative_returns, legend_loc='upper left', legend_ncol=1,\n",
    "            title='Daily Cummulative Returns', ylabel='Cummulative Return')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4090c6bc",
   "metadata": {},
   "source": [
    "On peut dès lors observer en 2020 un creux pour l'ensemble des stocks de notre échantillon, dû à la crise du covid, notamment Safran ou Airbus qui ont vu leur production particulièrement impactée par les confinements successifs. L'Oréal est l'entreprise qui a eu la tendance la plus haussières ces 10 dernières années et qui demeure être celle avec les plus gros rendements aujourd'hui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7155be4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ret = sample_returns.mean()*252\n",
    "cov_matrix = sample_returns.cov(numeric_only=True)*252\n",
    "cov_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ea351f",
   "metadata": {},
   "source": [
    "Nous allons utiliser la fonction ci-dessous pour générer aléatoirement les poids de notre portefeuille. Cette méthode permet de créer des vecteurs de poids contenant parfois des 0, ou des répartitions plus aléatoires que la fonction native random."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a2e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights_for_portfolio(stocks_number):\n",
    "    zero_vector = np.zeros(stocks_number)\n",
    "    random_weights_vector = zero_vector.copy()\n",
    "    num_zeros = np.random.randint(0, len(zero_vector) + 1)\n",
    "    \n",
    "    if num_zeros < len(zero_vector):\n",
    "        random_indices = np.random.choice(len(zero_vector), len(zero_vector) - num_zeros, replace=False)\n",
    "        for index in random_indices:\n",
    "            random_weights_vector[index] = np.random.random()\n",
    "    \n",
    "    # Normalize the weights so that their sum is equal to 1\n",
    "    total = np.sum(random_weights_vector)\n",
    "    if total != 0:\n",
    "        random_weights_vector = random_weights_vector / total\n",
    "    else:\n",
    "        random_weights_vector = np.full_like(random_weights_vector, 1 / len(random_weights_vector))\n",
    "    \n",
    "    return random_weights_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6609735",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_returns = sample_returns.std()*np.sqrt(252)\n",
    "\n",
    "assets_num=len(companies_names)\n",
    "port_num=200000\n",
    "port_ret=[]\n",
    "port_vol=[]\n",
    "port_ethic_score = []\n",
    "port_weights=[]\n",
    "sharpe_ratio=[]\n",
    "\n",
    "for portfolio in range(port_num):\n",
    "    weights = random_weights_for_portfolio(assets_num)\n",
    "    ret = np.dot(weights, mean_ret)\n",
    "    vol = np.sqrt(np.dot(weights.T, np.dot(cov_matrix,weights)))\n",
    "    ethic = np.dot(weights, sample_scores)\n",
    "    rf = 0.01\n",
    "    sharpe = (ret-rf)/vol\n",
    "    sharpe_ratio.append(sharpe)\n",
    "    port_ret.append(ret)\n",
    "    port_vol.append(vol)\n",
    "    port_weights.append(weights)\n",
    "    port_ethic_score.append(ethic)\n",
    "    \n",
    "# Dictionnaires contenant les données de chaque portefeuilles\n",
    "port_dict = {'Returns': port_ret,\n",
    "             'Volatility': port_vol, \n",
    "             'Sharpe Ratio': sharpe_ratio, \n",
    "             'Ethic Score' : port_ethic_score}\n",
    "\n",
    "for counter,symbol in enumerate(companies_ticker):\n",
    "    port_dict[symbol] = [Weight[counter] for Weight in port_weights]\n",
    "\n",
    "port_EF = pd.DataFrame(port_dict)\n",
    "\n",
    "gmv_port_value = port_EF['Volatility'].min()\n",
    "msr_port_value = port_EF['Sharpe Ratio'].max()\n",
    "\n",
    "gmv_port = port_EF.loc[port_EF['Volatility'] == gmv_port_value]\n",
    "msr_port = port_EF.loc[port_EF['Sharpe Ratio'] == msr_port_value]\n",
    "\n",
    "port_EF.plot.scatter(x='Volatility', y='Returns', c='Sharpe Ratio', cmap='viridis', edgecolors='black', figsize=(10, 8), grid=True)\n",
    "plt.xlabel('Volatility (Standard Deviation)')\n",
    "plt.ylabel('Expected Returns')\n",
    "plt.title('Portfolio Optimization: Efficient Frontier', weight='bold')\n",
    "plt.scatter(x=gmv_port['Volatility'], y=gmv_port['Returns'], c='blue', marker='*', s=500, label='Global Minimum Variance')\n",
    "plt.scatter(x=msr_port['Volatility'], y=msr_port['Returns'], c='red', marker='*', s=500, label='Maximum Sharpe Ratio')\n",
    "plt.legend(labelspacing=1.2)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f84a3",
   "metadata": {},
   "source": [
    "Ci-dessus nous avons donc simuler aléatoirement un grand nombre de portefeuilles pour créér un graphique 2D mettant en relation risque et rentabilitées. Par ailleurs nous avons affiché en bleu le portefeuille de variance (risque) minimale et en rouge le portfeuille des ratio de Sharpe minimal dont voici les poids : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e848c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_port.round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed977d5",
   "metadata": {},
   "source": [
    "Maintenant que nous avons tracé notre graphique en 2 dimensions (seulement avec le risque et les rentabilités). Tentons d'intégrer une nouvelle dimension: le score ESG de ces portefeuilles, obtenus en multipliant les poids par les scores respectifs de chaque entreprise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a06f5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "threshold = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3452236c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_3d(port_dict, x='Volatility', y='Returns', z='Ethic Score', color='Sharpe Ratio')\n",
    "fig.add_scatter3d(x=port_vol, y=port_ret, z=threshold*np.ones_like(port_vol), mode='lines', name='Threshold Plane', line=dict(color='black', width=2))\n",
    "\n",
    "fig.update_traces(marker=dict(size=5, opacity=0.8), selector=dict(mode='markers')) \n",
    "fig.update_layout(scene=dict(xaxis_title='Volatility', yaxis_title='Returns', zaxis_title='Ethic Score'))  \n",
    "fig.update_layout(coloraxis_colorbar=dict(title='Sharpe Ratio'))\n",
    "fig.update_layout(title='3D Scatter Plot', title_x=0.5) \n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d888ef38",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(port_EF[\"Volatility\"][port_EF[\"Ethic Score\"] > threshold], port_EF[\"Returns\"][port_EF[\"Ethic Score\"] > threshold], c=port_EF[\"Sharpe Ratio\"][port_EF[\"Ethic Score\"] > threshold], s=20, cmap='viridis')\n",
    "\n",
    "for i in range(len(companies_names)):\n",
    "    plt.scatter(std_returns[i], mean_ret[i], s=100, marker='o', label=companies_names[i])\n",
    "    \n",
    "plt.xlabel('Volatility (Standard Deviation)')\n",
    "plt.ylabel('Expected Returns')\n",
    "plt.title('Portfolio above the threshold', weight='bold')\n",
    "plt.legend(labelspacing=1.2)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c7ac3f",
   "metadata": {},
   "source": [
    "Ci-dessus, nous avons uniquement affiché les portefeuilles étant au dessus du seuil accordé aux entreprises en terme d'ESG. On remarque que le nombre de portefeuilles réalisables diminue considérablement. Nous pouvons des à présent afficher celui qui maximise le ratio de sharpe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38224d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_argmax_sr = port_EF[\"Sharpe Ratio\"].argmax()\n",
    "port_max_sr = port_EF[\"Sharpe Ratio\"].max()\n",
    "port_argmax_sr_w_esg = port_EF[\"Sharpe Ratio\"][port_EF[\"Ethic Score\"] > threshold].argmax()\n",
    "port_max_sr_w_esg = port_EF[\"Sharpe Ratio\"][port_EF[\"Ethic Score\"] > threshold].max()\n",
    "\n",
    "print(f\"Portefeuille maximisant le ratio de Sharpe (sans seuil ESG): \\n {port_EF.loc[port_EF['Sharpe Ratio'] == port_max_sr].iloc[:, 4:]}\")\n",
    "weights1 = port_EF.loc[port_EF['Sharpe Ratio'] == port_max_sr].iloc[:, 4:]\n",
    "print(f\"Rentabilité : {port_ret[port_argmax_sr]}\")\n",
    "print(f\"Volatilité : {port_vol[port_argmax_sr]}\")\n",
    "print(f\"Ratio de Sharpe : {port_max_sr}\")\n",
    "print(f\"Score ESG : {port_ethic_score[port_argmax_sr]}\")\n",
    "print(\"\\n\")\n",
    "\n",
    "print(f\"Portefeuille maximisant le ratio de Sharpe (avec seuil ESG): \\n {port_EF.loc[port_EF['Sharpe Ratio'] == port_max_sr_w_esg].iloc[:, 4:]}\")\n",
    "weights1_w_esg = port_EF.loc[port_EF['Sharpe Ratio'] == port_max_sr_w_esg].iloc[:, 4:]\n",
    "print(f\"Rentabilité : {port_ret[port_argmax_sr_w_esg]}\")\n",
    "print(f\"Volatilité : {port_vol[port_argmax_sr_w_esg]}\")\n",
    "print(f\"Ratio de Sharpe : {port_max_sr_w_esg}\")\n",
    "print(f\"Score ESG : {port_ethic_score[port_argmax_sr_w_esg]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86da59ac",
   "metadata": {},
   "source": [
    "On remarque donc bien une différence de composition du portefeuille si l'on considère ou non les actifs avec des bonnes notes ESG. On voit également que la rentabilité du portefeuille diminue si le score global ESG augmente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97a6112",
   "metadata": {},
   "source": [
    "### Méthode 2: Scipy optimizer and Bonus ESG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b91af7",
   "metadata": {},
   "source": [
    "L'idée ici est d'ajouter un bonus de rentabilité aux entreprises plus responsable et ayant un meilleurs score ESG. Cette manipulation est assez simple à mettre en place et permet de directement choisir ces investissement et son portefeuille avec 2 dimensions au final. Nous allons également utiliser l'optimizer natif de scipy pour optimiser nos poids de notre portefeuille. \n",
    "En premier lieu, il s'agit de déterminer quel bonus accorder à ces rentabilitées pour ne pas biaiser complètement nos résultats. Par exemple, accordons un bonus de rentabilité de 0.001 à toutes les entreprises ayant un score ESG supérieur ou égale à 0,6. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07591908",
   "metadata": {},
   "outputs": [],
   "source": [
    "bonus = 0.011\n",
    "\n",
    "temp_mean_ret = mean_ret\n",
    "for i in range(len(sample_scores)):\n",
    "    if sample_scores[i] > 0.6: \n",
    "        temp_mean_ret[i] += bonus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17d7e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def portfolio_return(weights, mean_ret): \n",
    "    return np.dot(weights, mean_ret)\n",
    "\n",
    "def portfolio_vol(weights, covmat):\n",
    "    return np.sqrt(np.dot(weights.T, np.dot(covmat,weights)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9984a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def minimize_vol(target_return, er, cov):\n",
    "    n = er.shape[0]\n",
    "    init_guess = np.repeat(1/n, n)\n",
    "    bounds = ((0.0, 1.0),)*n\n",
    "    return_is_target = {\n",
    "        'type' : 'eq',\n",
    "        'args' : (er,),\n",
    "        'fun' : lambda weights, er : target_return - portfolio_return(weights, er)\n",
    "    }\n",
    "    weights_sum_to_1 = {\n",
    "        'type' : 'eq',\n",
    "        'fun' : lambda weights : np.sum(weights) - 1\n",
    "    }\n",
    "    results = minimize(portfolio_vol, init_guess,\n",
    "                       args = (cov,), method = 'SLSQP',\n",
    "                       options={'disp': False},\n",
    "                       constraints= (return_is_target, weights_sum_to_1),\n",
    "                       bounds = bounds\n",
    "                      )\n",
    "    return results.x\n",
    "\n",
    "def optimal_weights(n_points, er, cov):\n",
    "    target_rs = np.linspace(er.min(), er.max(), n_points)\n",
    "    weights = [minimize_vol(target_return, er, cov) for target_return in target_rs]\n",
    "    return weights\n",
    "\n",
    "def plot_ef(n_points, er, cov):\n",
    "    weights = optimal_weights(n_points, er, cov)\n",
    "    rets = [portfolio_return(w, er) for w in weights]\n",
    "    vols = np.array([portfolio_vol(w, cov) for w in weights])\n",
    "    ef = pd.DataFrame({'Returns': rets, 'Volatility': vols})\n",
    "    fig = ef.plot(x='Volatility', y='Returns', style='.-', color='green')\n",
    "    \n",
    "    return fig\n",
    "\n",
    "plot_ef(1000, temp_mean_ret, cov_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebe34bd",
   "metadata": {},
   "source": [
    "L'objectif à présent est déterminer le portefeuille avec le ratio de Sharpe maximum, en traçant la ligne de marché (capital market line), qui est la droite tangente tracée depuis le point de l'actif sans risque jusqu'à la région réalisable pour les actifs risqués. Le point de tangence est le portefeuille avec le ratio de Sharpe maximal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2743521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msr(risk_free_rate, er, cov):\n",
    "    n = er.shape[0]\n",
    "    init_guess = np.repeat(1/n, n)\n",
    "    bounds = ((0.0, 1.0),)*n\n",
    "    weights_sum_to_1 = {\n",
    "        'type' : 'eq',\n",
    "        'fun' : lambda weights : np.sum(weights) - 1\n",
    "    }\n",
    "    \n",
    "    def neg_sharpe_ratio(weights, risk_free_rate, er, cov):\n",
    "        '''\n",
    "        Returns the negative sharpe ratio, given weights\n",
    "        '''\n",
    "        r = portfolio_return(weights, er)\n",
    "        vol = portfolio_vol(weights, cov)\n",
    "        return -(r - risk_free_rate)/vol\n",
    "    \n",
    "    results = minimize(neg_sharpe_ratio, init_guess,\n",
    "                       args = (risk_free_rate, er,cov,), method = 'SLSQP',\n",
    "                       options={'disp': False},\n",
    "                       constraints= (weights_sum_to_1),\n",
    "                        bounds = bounds\n",
    "                       )\n",
    "    return results.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ee1a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_msr = msr(0.1, temp_mean_ret, cov_matrix)\n",
    "ret_msr = portfolio_return(w_msr, temp_mean_ret)\n",
    "vol_msr = portfolio_vol(w_msr, cov_matrix)\n",
    "ax = plot_ef(1000, temp_mean_ret, cov_matrix)\n",
    "\n",
    "# Adding CML \n",
    "\n",
    "cml_x = [0, vol_msr]\n",
    "cml_y = [0.1, ret_msr]\n",
    "ax.plot(cml_x, cml_y, color='goldenrod', marker= 'x', markersize= 10)\n",
    "ax.set_xlim(left = 0)\n",
    "ax.set_title('Efficient Frontier and CML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0342a6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rentabilité et volatilité du portefeuille max ratio de sharpe : {ret_msr, vol_msr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be02893",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_decimal(value):\n",
    "    return float('{:.6f}'.format(value))\n",
    "\n",
    "weights_bonus = pd.DataFrame({'Weights bonus': w_msr}, index=sample_returns.columns).sort_values(by='Weights bonus')[:]\n",
    "weights_bonus = weights_bonus.applymap(format_decimal)\n",
    "weights_bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1780dc0c",
   "metadata": {},
   "source": [
    "On peut remarquer en conclusion que nous avons un portefeuille composé majoritaiement de quelques actions, à savoir L'Oréal, Safran ou encore Thalès, 2 entreprises ayant des excellentes notes ESG et de bonne rentabilités. De même, Thales, qui a une rentabilité égale à celle de L'Oréal à une place bien moins importante que celle-ci dans notre portefeuille, dûe à une note ESG faible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5add5429",
   "metadata": {},
   "source": [
    "Dans le même principe et pour étendre nos méthode, nous pourrions mettre en place un système de malus pour les entreprises qui possèdent des notes ESG faibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b05038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "malus = -0.01\n",
    "\n",
    "temp_mean_ret_malus = mean_ret\n",
    "for i in range(len(sample_scores)):\n",
    "    if sample_scores[i] < 0.5: \n",
    "        temp_mean_ret_malus[i] += malus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ac02db",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_msr = msr(0.1, temp_mean_ret_malus, cov_matrix)\n",
    "ret_msr = portfolio_return(w_msr, temp_mean_ret)\n",
    "vol_msr = portfolio_vol(w_msr, cov_matrix)\n",
    "ax = plot_ef(1000, temp_mean_ret_malus, cov_matrix)\n",
    "\n",
    "# Adding CML \n",
    "\n",
    "cml_x = [0, vol_msr]\n",
    "cml_y = [0.1, ret_msr]\n",
    "ax.plot(cml_x, cml_y, color='goldenrod', marker= 'x', markersize= 10)\n",
    "ax.set_xlim(left = 0)\n",
    "ax.set_title('Efficient Frontier and CML')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd2c1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_malus = pd.DataFrame({'Weights malus': w_msr}, index=sample_returns.columns).sort_values(by='Weights malus')[:]\n",
    "weights_malus = weights_malus.applymap(format_decimal)\n",
    "weights_malus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb2e971",
   "metadata": {},
   "source": [
    "On remarque dès lors que Thalès ayant une très bonne rentabilité n'apparait quasiment plus dans notre portefeuille. L'Oréal, ayant de bons rendements et une note éthique très haute devient majoritaire dans notre portefeuille."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57343a4e",
   "metadata": {},
   "source": [
    "### Méthode 3 : Optimisateur Scipy et contrainte ESG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b3d16b",
   "metadata": {},
   "source": [
    "Cette méthode va consister à simplement utiliser l'optimisateur de Scipy en ajoutant une contrainte ESG, par exemple une note ESG égale à 0.8, et minimiser la volatilité de la même manière que dans la méthode précédente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9397db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_vol_w_esg(target_return, er, cov, esg_score, threshold):\n",
    "    n = er.shape[0]\n",
    "    init_guess = np.repeat(1/n, n)\n",
    "    bounds = ((0.0, 1.0),)*n\n",
    "    return_is_target = {\n",
    "        'type' : 'eq',\n",
    "        'args' : (er,),\n",
    "        'fun' : lambda weights, er : target_return - portfolio_return(weights, er)\n",
    "    }\n",
    "    weights_sum_to_1 = {\n",
    "        'type' : 'eq',\n",
    "        'fun' : lambda weights : np.sum(weights) - 1\n",
    "    }\n",
    "    esg_score_to_threshold = {\n",
    "        'type' : 'eq',\n",
    "        'args' : (esg_score,),\n",
    "        'fun' : lambda weights, esg_score : portfolio_return(weights, esg_score) - threshold\n",
    "    }\n",
    "    results = minimize(portfolio_vol, init_guess,\n",
    "                       args = (cov,), method = 'SLSQP',\n",
    "                       options={'disp': False},\n",
    "                       constraints= (return_is_target, weights_sum_to_1, esg_score_to_threshold),\n",
    "                       bounds = bounds\n",
    "                      )\n",
    "    return results.x\n",
    "\n",
    "def optimal_weights_w_esg(n_points, er, cov, esg_score, threshold):\n",
    "    target_rs = np.linspace(er.min(), er.max(), n_points)\n",
    "    weights = [minimize_vol_w_esg(target_return, er, cov, esg_score, threshold) for target_return in target_rs]\n",
    "    return weights\n",
    "\n",
    "def plot_ef_w_esg(n_points, er, cov, esg_score):\n",
    "    threshold = 0.6\n",
    "    \n",
    "    weights_w_esg = optimal_weights_w_esg(n_points, er, cov, esg_score, threshold)\n",
    "    rets_w_esg = [portfolio_return(w, er) for w in weights_w_esg]\n",
    "    vols_w_esg = np.array([portfolio_vol(w, cov) for w in weights_w_esg])\n",
    "    \n",
    "    weights = optimal_weights(n_points, er, cov)\n",
    "    rets = [portfolio_return(w, er) for w in weights]\n",
    "    vols = np.array([portfolio_vol(w, cov) for w in weights])\n",
    "    \n",
    "    ef = pd.DataFrame({'Returns': rets, 'Volatility': vols})\n",
    "    ef_w_esg = pd.DataFrame({'Returns with ESG': rets_w_esg, 'Volatility with ESG': vols_w_esg})\n",
    "\n",
    "    fig = ef.plot(x='Volatility', y='Returns', style='.-', color='green')\n",
    "    ef_w_esg.plot(x='Volatility with ESG', y='Returns with ESG', style='.-', color='blue', ax=fig)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "plot_ef_w_esg(1000, mean_ret, cov_matrix, sample_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1e98ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msr_w_esg(risk_free_rate, er, cov, esg_score, threshold):\n",
    "    weights_w_esg = optimal_weights_w_esg(1000, er, cov, esg_score, threshold)\n",
    "    rets_w_esg = [portfolio_return(w, er) for w in weights_w_esg]\n",
    "    vols_w_esg = np.array([portfolio_vol(w, cov) for w in weights_w_esg])\n",
    "    sr = []\n",
    "    risk_free_rate = 0.1\n",
    "    for ret, vol in zip(rets_w_esg, vols_w_esg):\n",
    "        sr.append((ret - risk_free_rate)/vol)\n",
    "    index_max_rs = sr.index(max(sr))\n",
    "    return weights_w_esg[index_max_rs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5d3420",
   "metadata": {},
   "outputs": [],
   "source": [
    "w_msr_esg = msr_w_esg(0.1, mean_ret, cov_matrix, sample_scores, 0.6)\n",
    "weights_esg_constraint = pd.DataFrame({'Weights ESG constraint': w_msr_esg}, index=sample_returns.columns).sort_values(by='Weights ESG constraint')[:]\n",
    "weights_esg_constraint = weights_esg_constraint.applymap(format_decimal)\n",
    "weights_esg_constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1f6f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret_msr_esg = portfolio_return(w_msr_esg, mean_ret)\n",
    "vol_msr_esg = portfolio_vol(w_msr_esg, cov_matrix)\n",
    "ax = plot_ef_w_esg(1000, mean_ret, cov_matrix, sample_scores)\n",
    "\n",
    "# Adding CML \n",
    "\n",
    "cml_x = [0, vol_msr_esg]\n",
    "cml_y = [0.1, ret_msr_esg]\n",
    "ax.plot(cml_x, cml_y, color='goldenrod', marker= 'x', markersize= 10)\n",
    "ax.set_xlim(left = 0)\n",
    "ax.set_title('Efficient Frontier and CML')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107f7cf",
   "metadata": {},
   "source": [
    "Dans le même contexte, plutôt que de fixer une contrainte seuil pour notre score ESG, nous pourrions créer une fonction d'utilité à minimiser, dans le but de maximiser le score ESG tout en minimisant la volatilité à rentabilité égale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c157d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def utility_function(weights, cov, esg_score):\n",
    "    portfolio_volatility = portfolio_vol(weights, cov)\n",
    "    portfolio_esg_score = portfolio_return(weights, esg_score)\n",
    "\n",
    "    return portfolio_volatility - portfolio_esg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80325b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimize_vol_max_esg(target_return, er, cov, esg_score):\n",
    "    n = er.shape[0]\n",
    "    init_guess = np.repeat(1/n, n)\n",
    "    bounds = ((0.0, 1.0),)*n\n",
    "\n",
    "    utility_loss = lambda weights: utility_function(weights, cov, esg_score)\n",
    "\n",
    "    weights_sum_to_1 = {\n",
    "        'type' : 'eq',\n",
    "        'fun' : lambda weights : np.sum(weights) - 1\n",
    "    }\n",
    "    \n",
    "    return_is_target = {\n",
    "        'type' : 'eq',\n",
    "        'args' : (er,),\n",
    "        'fun' : lambda weights, er : target_return - portfolio_return(weights, er)\n",
    "    }\n",
    "\n",
    "    results = minimize(utility_loss, init_guess,\n",
    "                       method = 'SLSQP',\n",
    "                       options={'disp': False},\n",
    "                       constraints= (weights_sum_to_1, return_is_target),\n",
    "                       bounds = bounds\n",
    "                      )\n",
    "    return results.x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b8b351",
   "metadata": {},
   "source": [
    "On tente donc ici dans la fonction minimize_vol_max_esg de minimiser notre fonction utilité précédente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9646caf6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def optimal_weights_max_esg(n_points, er, cov, esg_score):\n",
    "    target_rs = np.linspace(er.min(), er.max(), n_points)\n",
    "    weights = [minimize_vol_max_esg(target_return, er, cov, esg_score) for target_return in target_rs]\n",
    "    return weights\n",
    "\n",
    "def plot_ef_max_esg(n_points, er, cov, esg_score):    \n",
    "    weights_max_esg = optimal_weights_max_esg(n_points, er, cov, esg_score)\n",
    "    rets_w_esg = [portfolio_return(w, er) for w in weights_max_esg]\n",
    "    vols_w_esg = np.array([portfolio_vol(w, cov) for w in weights_max_esg])\n",
    "    \n",
    "    weights = optimal_weights(n_points, er, cov)\n",
    "    rets = [portfolio_return(w, er) for w in weights]\n",
    "    vols = np.array([portfolio_vol(w, cov) for w in weights])\n",
    "    \n",
    "    ef = pd.DataFrame({'Returns': rets, 'Volatility': vols})\n",
    "    ef_w_esg = pd.DataFrame({'Returns with ESG': rets_w_esg, 'Volatility with ESG': vols_w_esg})\n",
    "\n",
    "    fig = ef.plot(x='Volatility', y='Returns', style='.-', color='green')\n",
    "    ef_w_esg.plot(x='Volatility with ESG', y='Returns with ESG', style='.-', color='blue', ax=fig)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "plot_ef_max_esg(1000, mean_ret, cov_matrix, sample_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "742887ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def msr_max_esg(risk_free_rate, er, cov, esg_score, threshold):\n",
    "    weights_max_esg = optimal_weights_max_esg(1000, er, cov, esg_score)\n",
    "    rets_max_esg = [portfolio_return(w, er) for w in weights_max_esg]\n",
    "    vols_max_esg = np.array([portfolio_vol(w, cov) for w in weights_max_esg])\n",
    "    sr = []\n",
    "    risk_free_rate = 0.1\n",
    "    for ret, vol in zip(rets_max_esg, vols_max_esg):\n",
    "        sr.append((ret - risk_free_rate)/vol)\n",
    "    index_max_rs = sr.index(max(sr))\n",
    "    return weights_max_esg[index_max_rs], max(sr)\n",
    "\n",
    "\n",
    "w_msr_max_esg = msr_max_esg(0.1, mean_ret, cov_matrix, sample_scores, 0.6)[0]\n",
    "weights_max_esg_constraint = pd.DataFrame({'Weights ESG constraint': w_msr_max_esg}, index=sample_returns.columns).sort_values(by='Weights ESG constraint')[:]\n",
    "weights_max_esg_constraint = weights_max_esg_constraint.applymap(format_decimal)\n",
    "weights_max_esg_constraint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd58724a",
   "metadata": {},
   "source": [
    "Pour conclure sur cette méthode, au vu de la frontière nous ne sommes pas sûr de la cohérence des résultats."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158c05a4",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599dacff",
   "metadata": {},
   "source": [
    "Observons de manière plus illustrative, nous allons comparer nos différentes méthodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5702b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Portolio weights\n",
    "msr = msr_port.iloc[:, 4:]\n",
    "\n",
    "def transform_df(df, name): \n",
    "    df_stacked = df.reset_index().melt(id_vars='index', var_name='Ticker', value_name=name)\n",
    "    df_stacked = df_stacked.drop('index', axis=1).set_index('Ticker')\n",
    "    return pd.DataFrame(df_stacked)\n",
    "\n",
    "\n",
    "\n",
    "msr_weights = transform_df(msr, 'Weights MSR')\n",
    "weights_method1 = transform_df(weights1, 'Weights Method1')\n",
    "weights_method1_w_esg = transform_df(weights1_w_esg, 'Weights Method1 esg')\n",
    "\n",
    "\n",
    "weights_df = pd.concat([msr_weights, weights_method1, weights_method1_w_esg, weights_bonus, weights_malus, weights_esg_constraint],axis=1)\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "colours = dict(zip(weights_df.index, plt.cm.tab10.colors[:len(weights_df.index)]))\n",
    "for column,num in zip(weights_df.columns, range(len(weights_df.columns))):\n",
    "    plt.subplot(1,6,num+1)\n",
    "    labels, values = zip(*((key,value) for key,value in weights_df[column].items() if value>0))\n",
    "    plt.pie(values, labels=labels, colors=[colours[key] for key in labels], autopct='%1.1f%%')\n",
    "    plt.title('{} Portfolio'.format(column), weight='bold')\n",
    "    fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407b680e",
   "metadata": {},
   "source": [
    "Pour conclure avec cette partie \"optimisation en 3D\", nous allons choisir une des méthodes que nous avons évoquée précédemment et la mettre en application dans le cadre construire un portefeuille.\n",
    "Maintenant, nous considérons que nous investissons 10 000€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56240fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investment amount\n",
    "total_investment = 10000\n",
    "bond_allocation_percentage = 0.7\n",
    "bond_annual_return = 0.1\n",
    "\n",
    "# Allocate a fraction of the investment to bonds\n",
    "bond_investment = total_investment * bond_allocation_percentage\n",
    "\n",
    "# Calculate the remaining amount for stock investment\n",
    "stock_investment = total_investment - bond_investment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed74345f",
   "metadata": {},
   "source": [
    "Si l'on choisit de prendre le portefeuille avec une note ESG minimale de 0.6 (méthode avec scipy et la contrainte ESG), on obtiendra alors un portefeuille de la valeur ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e61b02b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bond_investment_value = bond_investment * (1 + bond_annual_return)\n",
    "\n",
    "weights_transposed = weights_esg_constraint.transpose()\n",
    "stock_investment_value = (weights_transposed * stock_investment).sum()\n",
    "\n",
    "total_investment_value = bond_investment_value + stock_investment_value\n",
    "\n",
    "print(\"Valeur totale de l'investissement après un an :\\n\", total_investment_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "744aaf5d",
   "metadata": {},
   "source": [
    "### Gestion indicielle et inflation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdd07e6",
   "metadata": {},
   "source": [
    "Notre objectif va maintenant être de tenter de suivre l'inflation. C'est-à-dire de non-plus construire un portefeuille qui minimise la volatilité ou maximise la rentabilité, mais cette fois de construire un portefeuille qui minimise la tracking error par rapport à notre benchmark, qui est ici l'inflation. Notre idée va être de tester nos deux types de portfeuilles, avec ou sans contrainte ESG. Dans un premier il s'agit de définir notre fonction à minimiser. On sait que pour une gestion indicielle, notre système à optimiser a la forme :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0049b4a",
   "metadata": {},
   "source": [
    "\\begin{cases}\n",
    "\\min_{x_{t}} ((x_{t} - C)^{T}V(x_{t} - C)) \\\\\n",
    "x_{t}^{T} 1_{N} = 1\n",
    "\\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ac2e2b",
   "metadata": {},
   "source": [
    "avec C composition de l'actif et V la matrice de covariance des rentabilités"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35dcec69",
   "metadata": {},
   "source": [
    "Pour suivre l'inflation nous avons trouvé un ETF qui suit l'inflation en zone euro (on part cette fois-ci de 2021): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f9059",
   "metadata": {},
   "outputs": [],
   "source": [
    "etf_inflation = yf.download('0WAP.L', start=\"2021-01-31\", end=\"2024-01-15\")['Close']\n",
    "etf_inflation = etf_inflation.interpolate(method=\"nearest\")\n",
    "etf_inflation.tail().round(1)\n",
    "\n",
    "# On calcule les rendements\n",
    "inflation_returns = etf_inflation.pct_change().dropna()\n",
    "\n",
    "returns_from_2021 = sample_returns.loc['2021-01-31':]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "inflation_returns.plot(color='blue', lw=1, label='ETF sur l\\'inflation (0WAP.L)')\n",
    "plt.title('Performance de l\\'ETF sur l\\'inflation')\n",
    "returns_from_2021.plot(color='blue', lw=1)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Prix de clôture')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d8affc",
   "metadata": {},
   "source": [
    "On peut remarquer au vu du tracé quelques similarités notamment au niveaux des pics. Par la suite créons les différents vecteurs squi nous servirons à optimiser notre portefeuille."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee80de32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rentas annuelles de l'inflation\n",
    "mean_ret_inflation = inflation_returns.mean()*252 \n",
    "\n",
    "#Matrice C \n",
    "C = np.zeros(len(mean_ret) + 1)\n",
    "C[-1] = 1\n",
    "\n",
    "# vecteur des rentabilités \n",
    "returns_w_inflation = {ticker: round(value, 5) for ticker, value in mean_ret.items()}\n",
    "returns_w_inflation['Inflation'] = round(mean_ret_inflation,5)\n",
    "returns_w_inflation = pd.Series(returns_w_inflation, name='Rentabilités')\n",
    "\n",
    "\n",
    "# poids portefeuille\n",
    "inflation_row = pd.DataFrame({'Weights MSR': [0.0]}, index=['Inflation'])\n",
    "X = msr_weights\n",
    "X = X.append(inflation_row)\n",
    "\n",
    "# matrice covariance \n",
    "returns_df = returns_from_2021\n",
    "inflation_returns = pd.DataFrame(inflation_returns)\n",
    "inflation_returns = inflation_returns.rename(columns={'Close': 'Inflation'})\n",
    "\n",
    "returns_df = pd.concat([returns_from_2021, inflation_returns],axis=1).dropna()\n",
    "cov_matrix_inflation = returns_df.cov(numeric_only=True)*252"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3fc73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tracking_error(weights, cov):\n",
    "    x_minus_C_transpose = (weights - C).reshape(1, -1)\n",
    "    return np.dot(x_minus_C_transpose, np.dot(cov, (weights - C)))\n",
    "    \n",
    "def minimize_vol_inflation(er, cov):\n",
    "    n = er.shape[0]\n",
    "    init_guess = np.repeat(1/n, n)\n",
    "    init_guess[-1] = 0.0\n",
    "\n",
    "    bounds = ((0.0, 1.0),) * (n - 1) + ((0.0, 0.0),)\n",
    "    weights_sum_to_1 = {\n",
    "        'type' : 'eq',\n",
    "        'fun' : lambda weights : np.sum(weights) - 1\n",
    "    }\n",
    "    results = minimize(tracking_error, init_guess,\n",
    "                       args = (cov,), method = 'SLSQP',\n",
    "                       options={'disp': False},\n",
    "                       constraints= (weights_sum_to_1),\n",
    "                       bounds = bounds\n",
    "                      )\n",
    "    return results.x\n",
    "\n",
    "def optimal_weights_w_inflation(er, cov):\n",
    "    weights = minimize_vol_inflation(er, cov)\n",
    "    return weights\n",
    "\n",
    "weights_inflation = optimal_weights_w_inflation(returns_w_inflation, cov_matrix_inflation)\n",
    "weights_inflation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bfd043",
   "metadata": {},
   "outputs": [],
   "source": [
    "ret = portfolio_return(weights_inflation, returns_w_inflation)\n",
    "vol = portfolio_return(weights_inflation, cov_matrix_inflation)\n",
    "\n",
    "weights_inflation_benchmark = pd.DataFrame({'Weights inflation benchmark': weights_inflation}, index=returns_df.columns).sort_values(by='Weights inflation benchmark')[:]\n",
    "weights_inflation_benchmark = weights_inflation_benchmark.applymap(format_decimal)\n",
    "\n",
    "weights_inflation_benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddeb2cb",
   "metadata": {},
   "source": [
    "## Optimisation en 4D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b2c19e",
   "metadata": {},
   "source": [
    "Nous allons à présent inclure dans nos investissements la possibilité d'investir dans de l'or et du franc suisse. L'idée est de concevoir une situation de Krach boursier et d'optimiser notre portefeuille pour répondre au mieux à cette inadvertance. Dans un premier temps simuler de la manière la plus simple possible un scénario de krach aléatoirement sur nos 10 stocks et tentons d'optimiser notre portefeuille. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0009a3be",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Définition des rendements en cas de krach pour chaque action\n",
    "krach_returns_data = {\n",
    "    \"SAN.PA\": -0.2, \n",
    "    \"AI.PA\": -0.3,  \n",
    "    \"HO.PA\": -0.3,  \n",
    "    \"OR.PA\": -0.4, \n",
    "    \"BNP.PA\": -0.4, \n",
    "    \"AIR.PA\": -0.2, \n",
    "    \"BN.PA\": -0.3, \n",
    "    \"CS.PA\": -0.4, \n",
    "    \"SAF.PA\": -0.4,  \n",
    "    \"TTE.PA\": -0.2,  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c6f275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la perte potentielle en cas de krach\n",
    "def krach_loss(weights, krach_returns):\n",
    "    krach_portfolio_ret = np.dot(weights, list(krach_returns.values()))\n",
    "    loss = -krach_portfolio_ret\n",
    "    return loss\n",
    "\n",
    "def optimize_portfolio_krach(krach_returns):\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    bounds = tuple((0, 1) for _ in range(len(krach_returns)))\n",
    "    init_weights = [1/len(krach_returns)] * len(krach_returns)\n",
    "    optimal_weights = minimize(krach_loss, init_weights, args = (krach_returns,), method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "    return optimal_weights.x\n",
    "\n",
    "optimal_weights = optimize_portfolio_krach(krach_returns_data)\n",
    "print(\"Pondérations optimales du portefeuille pour minimiser la perte en cas de krach :\")\n",
    "for company, weight in zip(companies_names, optimal_weights):\n",
    "    print(f\"{company}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5dc230",
   "metadata": {},
   "source": [
    "Sans contrainte ESG et après le krach, on peut voir que notre portefeuille est constitué majoritairement des stocks ayant été les moins exposés aux baisses des rentabilités."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8466a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_portfolio_krach_w_esg(esg_score, krach_returns):\n",
    "    threshold = 0.7\n",
    "    weights_constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    esg_score_to_threshold = {\n",
    "        'type' : 'eq',\n",
    "        'args' : (esg_score,),\n",
    "        'fun' : lambda weights, esg_score : portfolio_return(weights, esg_score) - threshold\n",
    "    }\n",
    "    bounds = tuple((0, 1) for _ in range(len(krach_returns)))\n",
    "    init_weights = [1/len(krach_returns)] * len(krach_returns)\n",
    "    optimal_weights = minimize(krach_loss, init_weights, args = (krach_returns,), method='SLSQP', bounds=bounds, constraints=(weights_constraints, esg_score_to_threshold))\n",
    "    return optimal_weights.x\n",
    "\n",
    "optimal_weights_w_esg = optimize_portfolio_krach_w_esg(sample_scores, krach_returns_data)\n",
    "print(\"Pondérations optimales du portefeuille pour minimiser la perte en cas de krach avec contraintes ESG:\")\n",
    "for company, weight in zip(companies_names, optimal_weights_w_esg):\n",
    "    print(f\"{company}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e4d8e1",
   "metadata": {},
   "source": [
    "On remarque que notre portefeuille est cette fois-ci constituer de seulement 2 actifs, L'Oréal (pas présent dans le portefeuille précédent) qui a une note éthique très élevée, et Airbus, qui voit son rendement diminuer de seulement 0.2 et que a une note éthique plutôt convenable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ce6315",
   "metadata": {},
   "source": [
    "Nous allons maintenant illustrer ces krach boursiers de manière plus réaliste. En effet, plutôt que de diminuer nos rentabilités annuelles moyennes directement, ce qui semble demesuré. Nous allons insérer dans notre dataframe des rendements quelques lignes (plusieurs dates futures simulées), qui vont illustrer un krach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0387f0e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Création de 2 nouvelles dates où le krach apparait\n",
    "future_dates = pd.date_range(start='2024-01-13', periods=2, freq='D')  # Nouvelles dates\n",
    "\n",
    "np.random.seed(42)\n",
    "krach_returns = pd.DataFrame(np.random.uniform(-0.2, -0.3, size=(2, 10)), columns=sample_returns.keys(), index=future_dates)\n",
    "\n",
    "data_with_krach = pd.concat([pd.DataFrame(sample_returns), krach_returns])\n",
    "data_with_krach.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143fcab6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_ret_krach = data_with_krach.mean()*252\n",
    "mean_ret_krach_dict = {ticker: round(value, 4) for ticker, value in mean_ret_krach.items()}\n",
    "\n",
    "optimal_weights_w_esg = optimize_portfolio_krach_w_esg(sample_scores, mean_ret_krach_dict)\n",
    "print(\"Pondérations optimales du portefeuille pour minimiser la perte en cas de krach avec contraintes ESG:\")\n",
    "for company, weight in zip(companies_names, optimal_weights_w_esg):\n",
    "    print(f\"{company}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c75c08a",
   "metadata": {},
   "source": [
    "On retrouve encore une fois après le krach et sous contrainte ESG un portefeuille constitué de L'Oréal en majorité, du fait de son score éthique élevé. Cependant si l'on refait notre simulation avec des données de krach différentes (seed qui change), on s'aperçoit que notre portefeuille a composition bien différente, avec toujours L'Oréal, mais également les stocks qui ont été le moins impacté par le krach et ayant des scores ESG au dessus du threshold fixé lors de l'optimisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e6a213",
   "metadata": {},
   "source": [
    "Dorénavant nous allons ajouté la possibilité d'investir dans de l'or ou du franc suisse. Nous allons donc récupérer leurs données historiques grâce à yfinance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69af6b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_data = yf.download('GC=F', start=\"2020-01-31\", end=\"2024-01-15\")['Close']\n",
    "\n",
    "# On interpole les données manquantes\n",
    "gold_data = gold_data.interpolate(method=\"nearest\")\n",
    "gold_data.tail().round(1)\n",
    "\n",
    "# On calcule les rendements de l'or\n",
    "sample_returns_gold = gold_data.pct_change().dropna()\n",
    "sample_returns_gold = pd.DataFrame(sample_returns_gold)\n",
    "sample_returns_gold = sample_returns_gold.rename(columns={'Close': 'Gold'})\n",
    "\n",
    "sample_returns_w_gold = pd.concat([sample_returns, sample_returns_gold],axis=1)\n",
    "sample_returns_w_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b427e4e",
   "metadata": {},
   "source": [
    "Une des premières étapes consiste à ajouter une note ESG à notre or. Nous allons choisir la note par défaut de 0.0, car cet actif ne rentre dans aucun critère ESG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb6ea17",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_scores_w_gold_dict = {company: round(value, 5) for company, value in sample_scores.items()}\n",
    "sample_scores_w_gold_dict['Gold'] = 0.0\n",
    "sample_scores_w_gold = pd.Series(sample_scores_w_gold_dict, name='Score global')\n",
    "companies_names.append('gold')\n",
    "sample_scores_w_gold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44863c00",
   "metadata": {},
   "source": [
    "Maintenant nous savons que le marché de l'or est décorrélé du marché des actions. L'or est traditionnellement considéré comme un actif refuge en période d'incertitude économique ou de crise financière. Les investisseurs cherchent souvent à investir dans l'or lorsque les marchés boursiers connaissent des turbulences, car l'or est perçu comme une valeur refuge qui maintient sa valeur dans des conditions économiques difficiles. Il nous faut donc ajouter cette information dans notre simulation de krach et donc faire augmenter très largement les rendements de l'or (environ 40%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732ab959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de 2 nouvelles dates où le krach apparait\n",
    "future_dates = pd.date_range(start='2024-01-13', periods=2, freq='D')  # Nouvelles dates\n",
    "\n",
    "np.random.seed(42)\n",
    "krach_returns_stocks = np.random.uniform(-0.2, -0.3, size=(2, 10))\n",
    "krach_return_gold = np.random.uniform(0.4, 0.5, size=(2, 1)) \n",
    "\n",
    "krach_returns = pd.DataFrame(np.hstack((krach_returns_stocks, krach_return_gold)), \n",
    "                              columns=sample_returns_w_gold.columns, index=future_dates)\n",
    "\n",
    "data_w_gold_with_krach = pd.concat([sample_returns_w_gold, krach_returns])\n",
    "data_w_gold_with_krach.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d907ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ret_krach_w_gold = data_w_gold_with_krach.mean()*252\n",
    "mean_ret_krach_w_gold_dict = {ticker: round(value, 4) for ticker, value in mean_ret_krach_w_gold.items()}\n",
    "\n",
    "optimal_weights_w_gold_esg = optimize_portfolio_krach_w_esg(sample_scores_w_gold, mean_ret_krach_w_gold_dict)\n",
    "print(\"Pondérations optimales du portefeuille pour minimiser la perte en cas de krach avec contraintes ESG:\")\n",
    "for company, weight in zip(companies_names, optimal_weights_w_gold_esg):\n",
    "    print(f\"{company}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab26a576",
   "metadata": {},
   "source": [
    "En considérant l'ESG dans notre optimisation, on peut voir que notre portefeuille est à présent uniquement composer de l'action L'Oréal, grâce à sa forte note ESG, mais également de l'or, qui voit ses rendements exploser. Si l'on optimise sans ESG notre portefeuille en simulant ce même krach, celui-ci sera quasiment uniquement composé d'or."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad820903",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_weights_w_gold = optimize_portfolio_krach(mean_ret_krach_w_gold_dict)\n",
    "\n",
    "print(\"Pondérations optimales du portefeuille pour minimiser la perte en cas de krach:\")\n",
    "for company, weight in zip(companies_names, optimal_weights_w_gold):\n",
    "    print(f\"{company}: {weight:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5df142",
   "metadata": {},
   "source": [
    "Pour finir, nous allons tenter de minimser d'autre fonctions de perte pour se baser sur plusieurs métriques de risques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e9dc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction pour calculer la VaR à 95%\n",
    "def var_loss(weights, returns):\n",
    "    weighted_returns = weights * np.array(list(returns.values()))\n",
    "    loss = -np.percentile(weighted_returns, 5) \n",
    "    return loss\n",
    "\n",
    "# Fonction pour calculer le drawdown\n",
    "def drawdown_loss(weights, returns):\n",
    "    wealth_index = 1000 * (1 + pd.Series(returns).cumprod())\n",
    "    previous_peaks = wealth_index.cummax()\n",
    "    drawdowns = (wealth_index - previous_peaks) / previous_peaks\n",
    "    max_drawdown = drawdowns.min()\n",
    "    return max_drawdown\n",
    "\n",
    "# Fonctions pour optimiser les poids du portefeuille pour minimiser la VaR et le drawdown\n",
    "def optimize_portfolio_var(esg_score, returns, with_esg):\n",
    "    weights_constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    esg_score_to_threshold = {\n",
    "        'type' : 'eq',\n",
    "        'args' : (esg_score,),\n",
    "        'fun' : lambda weights, esg_score : portfolio_return(weights, esg_score) - threshold\n",
    "    }\n",
    "    bounds = tuple((0, 1) for _ in range(len(returns)))\n",
    "    init_weights = [1 / len(returns)] * len(returns)\n",
    "    if (with_esg) :\n",
    "        optimal_weights = minimize(var_loss, init_weights, args=(returns,), method='SLSQP', bounds=bounds, constraints=(weights_constraints,esg_score_to_threshold))\n",
    "    else : \n",
    "        optimal_weights = minimize(var_loss, init_weights, args=(returns,), method='SLSQP', bounds=bounds, constraints=weights_constraints)\n",
    "    return optimal_weights.x\n",
    "\n",
    "\n",
    "def optimize_portfolio_drawdown(esg_score, returns, with_esg):\n",
    "    constraints = ({'type': 'eq', 'fun': lambda x: np.sum(x) - 1})\n",
    "    esg_score_to_threshold = {\n",
    "        'type' : 'eq',\n",
    "        'args' : (esg_score,),\n",
    "        'fun' : lambda weights, esg_score : portfolio_return(weights, esg_score) - threshold\n",
    "    }\n",
    "    bounds = tuple((0, 1) for _ in range(len(returns)))\n",
    "    init_weights = [1 / len(returns)] * len(returns)\n",
    "    if (with_esg) : \n",
    "        optimal_weights = minimize(drawdown_loss, init_weights, args=(returns,), method='SLSQP', bounds=bounds, constraints=(constraints, esg_score_to_threshold))\n",
    "    else : \n",
    "        optimal_weights = minimize(drawdown_loss, init_weights, args=(returns,), method='SLSQP', bounds=bounds, constraints=constraints )\n",
    "    return optimal_weights.x\n",
    "\n",
    "optimal_weights_var = optimize_portfolio_var(sample_scores_w_gold, mean_ret_krach_w_gold_dict, True)\n",
    "optimal_weights_drawdown = optimize_portfolio_drawdown(sample_scores_w_gold, mean_ret_krach_w_gold_dict, True)\n",
    "print(\"Pondérations optimales du portefeuille pour minimiser le drawdown en cas de krach:\")\n",
    "for company, weight in zip(companies_names, optimal_weights_var):\n",
    "    print(f\"{company}: {weight:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456ba542",
   "metadata": {},
   "source": [
    "En fixant la contrainte ESG et en minimisant la Value at Risk, on peut remarquer une nouvelle composition de notre portefeuille, avec toujours l'Oréal comme composante principale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a861a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
